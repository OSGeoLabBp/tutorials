{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "img_def.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OSGeoLabBp/tutorials/blob/master/english/data_processing/lessons/img_def.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Movement and deformation analysis from images"
      ],
      "metadata": {
        "id": "5gFgDXngEaQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Principles\n",
        "\n",
        "*   Images/videos are made by a stable camera, to put it another way, the camera does not move during observations\n",
        "*   Calibrated camera/system is necessary\n",
        "*   Image resolution is enhanced by geodetic telescope\n"
      ],
      "metadata": {
        "id": "qqohFsdNGeZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Methods\n",
        "\n",
        "*   Template matching\n",
        "*   Pattern recognition\n",
        "\n",
        "###Template matching characteristics\n",
        "\n",
        "Pros\n",
        "\n",
        "*   There is always a match\n",
        "*   Simple algorithm\n",
        "*   Special marker is not necessary\n",
        "\n",
        "Cons\n",
        "\n",
        "*   The chance of false match is higher\n",
        "*   No or minimal rotation\n",
        "*   No or minimal scale change\n",
        "\n",
        "###Pattern recognition charasteristics\n",
        "\n",
        "Pros\n",
        "\n",
        "*   Marker can rotate\n",
        "*   Marker scale can change\n",
        "*   Normal of the marker can be estimated\n",
        "\n",
        "Cons\n",
        "\n",
        "\n",
        "*   Special marker have to be fit to target\n",
        "*   More sensitive for light conditions\n"
      ],
      "metadata": {
        "id": "hjkC7zS6H90r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First off, let's import the necessary Python packages."
      ],
      "metadata": {
        "id": "RZgJNP_kSZtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob                         # to extend file name pattern to list\n",
        "import cv2                          # OpenCV for image processing\n",
        "import numpy as np                  # for matrices\n",
        "import matplotlib.pyplot as plt     # to show images\n",
        "cv2.__version__"
      ],
      "metadata": {
        "id": "FXOBDJnKR5Jd",
        "outputId": "178d9155-5079-4da2-a96a-654a1f22f4a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "No module named 'cv2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cbbc1ccd1fc6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m                         \u001b[0;31m# to extend file name pattern to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m                          \u001b[0;31m# OpenCV for image processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m                  \u001b[0;31m# for matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m     \u001b[0;31m# to show images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mpreviously_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mmodule_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mcv_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodule_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Break out of outer loop when breaking out of inner loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: No module named 'cv2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Template matching"
      ],
      "metadata": {
        "id": "lq7p_fr5PRQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first download an image and a template to search for. The template is a smaller part of the original image."
      ],
      "metadata": {
        "id": "zpAUz4llR-UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O sample_data/monalisa.jpg https://raw.githubusercontent.com/OSGeoLabBp/tutorials/master/english/img_processing/code/monalisa.jpg\n",
        "!wget -q -O sample_data/mona_temp4.png https://raw.githubusercontent.com/OSGeoLabBp/tutorials/master/english/img_processing/code/mona_temp4.png"
      ],
      "metadata": {
        "id": "9wXy7SQ8RpiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the image used for processing and the template are converted to grayscale images to boost efficiency."
      ],
      "metadata": {
        "id": "dAgcu4bLS5N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('sample_data/monalisa.jpg')          # load image\n",
        "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)      # convert image to grayscale\n",
        "templ = cv2.imread('sample_data/mona_temp4.png')      # load template\n",
        "templ_gray = cv2.cvtColor(templ, cv2.COLOR_BGR2GRAY)  # convert template to grayscale\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) # show image and template\n",
        "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('image to scan')\n",
        "ax2.imshow(cv2.cvtColor(templ, cv2.COLOR_BGR2RGB))    # BGR vs. RGB\n",
        "ax2.set_title('template to find')\n",
        "ax2.set_xlim(ax1.get_xlim())                          # set same scale\n",
        "ax2.set_ylim(ax1.get_ylim())\n",
        "print(f'image sizes: {img_gray.shape}  template sizes: {templ_gray.shape}')"
      ],
      "metadata": {
        "id": "GATH0ws0P8B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Change the code above to plot grayscale images.*"
      ],
      "metadata": {
        "id": "CduEkw_OXjbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OpenCV package has a function for template mathing, so let's call it and display the result. The *matchTemplate* function can calculate six different formulas to find the best match. Within the function, *TM_CCOEFF_NORMED* it calculates a normalized coefficient in the range (0, 1), where the perfect match gives value 1."
      ],
      "metadata": {
        "id": "v3WhnNyJXuUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = cv2.matchTemplate(img_gray, templ_gray, cv2.TM_CCOEFF_NORMED)\n",
        "val, _, max = cv2.minMaxLoc(result)[1:4]        # get position of best match\n",
        "fr = np.array([max,\n",
        "               (max[0]+templ.shape[1], max[1]),\n",
        "               (max[0]+templ.shape[1], max[1]+templ.shape[0]),\n",
        "               (max[0], max[1]+templ.shape[0]),\n",
        "               max])\n",
        "result_uint = ((result - np.min(result)) / (np.max(result) - np.min(result)) * 256).astype('uint8')\n",
        "result_img = cv2.cvtColor(result_uint, cv2.COLOR_GRAY2BGR)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Match on original image')\n",
        "ax1.plot(fr[:,0], fr[:,1], 'r')\n",
        "ax1.plot([max[0]],[max[1]], 'r*')\n",
        "ax2.imshow(result_img)\n",
        "ax2.plot(fr[:,0], fr[:,1], 'r')\n",
        "ax2.plot([max[0]],[max[1]], 'r*')\n",
        "ax2.set_title('Normalized coefficients')\n",
        "ax2.set_xlim(ax1.get_xlim())                          # set same scale\n",
        "ax2.set_ylim(ax1.get_ylim())\n",
        "print(f'best match at {max} value {val:.6f}')\n"
      ],
      "metadata": {
        "id": "e9U69Aq7eB3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Change the code above and try other methods, TM_CCORR_NORMED, TM_SQDIFF_NORMED, for instance.*"
      ],
      "metadata": {
        "id": "KE5k2f_xXtn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Image transformation"
      ],
      "metadata": {
        "id": "LXEBkuP6Lurv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the pattern is rotated or scaled, the pattern might not match the image. This issue can be fixed by using homology matrix. For more details see: [source](https://github.com/jephraim-manansala/object-detection/blob/master/Object%20Detection%20using%20Template%20Matching.ipynb)\n",
        "\n",
        "Let's download another image with a rotated Mona Lisa."
      ],
      "metadata": {
        "id": "Vfp-1elpMQoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O sample_data/monalisa_tilt.jpg https://raw.githubusercontent.com/OSGeoLabBp/tutorials/master/english/img_processing/code/monalisa_tilt.jpg"
      ],
      "metadata": {
        "id": "0E0yWO_cObCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to find the template on the rotated image."
      ],
      "metadata": {
        "id": "7hSmp5_yy-pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('sample_data/monalisa_tilt.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "result = cv2.matchTemplate(img, templ_gray, cv2.TM_CCOEFF_NORMED)\n",
        "val, _, max = cv2.minMaxLoc(result)[1:4]\n",
        "fr = np.array([max,\n",
        "               (max[0]+templ.shape[1], max[1]),\n",
        "               (max[0]+templ.shape[1], max[1]+templ.shape[0]),\n",
        "               (max[0], max[1]+templ.shape[0]),\n",
        "               max])\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.plot(fr[:,0], fr[:,1], 'r')\n",
        "plt.plot([max[0]],[max[1]], 'r*')\n",
        "print(f'best match at {max} value {val:.6f} BUT FALSE!')"
      ],
      "metadata": {
        "id": "SLfZJGAIdWU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's transform the image back to the perpendicular plan."
      ],
      "metadata": {
        "id": "WLcIKRexscdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_img(image, a_src, a_dst):\n",
        "    \"\"\" calculate transformation matrix \"\"\"\n",
        "\n",
        "    new_image = image.copy()                       # make a copy of input image\n",
        "    # get parameters of transformation\n",
        "    projective_matrix = cv2.getPerspectiveTransform(a_src, a_dst)\n",
        "    # transform image\n",
        "    transformed = cv2.warpPerspective(img, projective_matrix, image.shape)\n",
        "    # cut destination area\n",
        "    transformed = transformed[0:int(np.max(a_dst[:,1])),0:int(np.max(a_dst[:,0]))]\n",
        "    return transformed"
      ],
      "metadata": {
        "id": "FKgVi_GaZyWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frame on warped image\n",
        "src = [(240, 44), (700, 116), (703, 815), (243, 903)]\n",
        "# frame on original\n",
        "s = img_gray.shape\n",
        "dst = [(0, 0), (s[1], 0), (s[1], s[0]), (0,s[0])]\n",
        "a_src = np.float32(src)\n",
        "a_dst = np.float32(dst)\n",
        "# image transformation\n",
        "img_dst = project_img(img, a_src, a_dst)\n",
        "# template match\n",
        "result = cv2.matchTemplate(img_dst, templ_gray, cv2.TM_CCOEFF_NORMED)\n",
        "val, _, max = cv2.minMaxLoc(result)[1:4]\n",
        "# frame around template on transformed image\n",
        "fr = np.array([max,\n",
        "               (max[0]+templ.shape[1], max[1]),\n",
        "               (max[0]+templ.shape[1], max[1]+templ.shape[0]),\n",
        "               (max[0], max[1]+templ.shape[0]),\n",
        "               max])\n",
        "fig, ax = plt.subplots(1,2, figsize=(13,8))\n",
        "ax[0].imshow(img, cmap=\"gray\");\n",
        "ax[0].plot(a_src[:,0], a_src[:,1], 'r--')\n",
        "ax[0].set_title('Original Image')\n",
        "ax[1].imshow(img_dst, cmap=\"gray\")\n",
        "ax[1].plot(a_dst[:,0], a_dst[:,1], 'r--')\n",
        "ax[1].set_title('Warped Image')\n",
        "ax[1].plot(fr[:,0], fr[:,1], 'r')\n",
        "ax[1].plot([max[0]],[max[1]], 'r*')\n",
        "print(f'best match at {max} value {val:.2f}')"
      ],
      "metadata": {
        "id": "3y0VdaYJaJNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Recognition of ArUco markers"
      ],
      "metadata": {
        "id": "9eeTJ-pp-BFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"An ArUco marker is a synthetic square marker composed by a wide black border and an inner binary matrix which determines its identifier (id). The black border facilitates its fast detection in the image and the binary codification allows its identification and the application of error detection and correction techniques. The marker size determines the size of the internal matrix. For instance a marker size of 4x4 is composed by 16 bits.\" (*from OpenCV documentation*)\n",
        "\n",
        "There is a contrib package in OpenCV to detect ArUco markers called *aruco*."
      ],
      "metadata": {
        "id": "tkpoP0joF7X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find six ArUco markers on a simple image."
      ],
      "metadata": {
        "id": "GfAPyBYD17lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O sample_data/markers.png https://raw.githubusercontent.com/OSGeoLabBp/tutorials/master/english/img_processing/code/markers.png\n",
        "img = cv2.imread('sample_data/markers.png')\n",
        "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
      ],
      "metadata": {
        "id": "OHx8wJMlGui4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_100)\n",
        "params = cv2.aruco.DetectorParameters()\n",
        "detector = cv2.aruco.ArucoDetector(aruco_dict, params)\n",
        "corners, ids, _ = detector.detectMarkers(img_gray)\n",
        "x = np.zeros(ids.size)\n",
        "y = np.zeros(ids.size)\n",
        "img1 = img.copy()\n",
        "for j in range(ids.size):\n",
        "  x[j] = int(round(np.average(corners[j][0][:, 0])))\n",
        "  y[j] = int(round(np.average(corners[j][0][:, 1])))\n",
        "  cv2.putText(img1, str(ids[j][0]), (int(x[j]+2), int(y[j])), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 255), 3)\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
        "ax[0].imshow(img)\n",
        "ax[1].imshow(img1)\n",
        "ax[1].plot(x, y, \"ro\")\n",
        "print(list(zip(list(x), list(y))))"
      ],
      "metadata": {
        "id": "4Avg2IfMKCsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calibration\n",
        "\n",
        "Low-cost cameras might have significant distortions (either radial or tangential). Therefore, we have to calibrate cameras before using in deformation and movement analysis.\n",
        "\n",
        "###Radial distortion\n",
        "\n",
        "$$ x' = x (1 + k_1 r^2 + k_2 r^4 + k_3 r^6) $$\n",
        "$$ y' = y (1 + k_1 r^2 + k_2 r^4 + k_3 r^6) $$\n",
        "\n",
        "###Tangential distortion\n",
        "\n",
        "$$ x' = x + (2 p_1 x y + p_2 (r^2 + 2 x^2)) $$\n",
        "$$ y' = y + (p_1 (r^2+2 y^2) + 2 p_2 x y) $$\n",
        "\n",
        "###Camera matrix\n",
        "\n",
        "<table>\n",
        "<tr><td>f<sub>x</sub></td><td>0</td><td>c<sub>x</sub></td></tr>\n",
        "<tr><td>0</td><td>f<sub>y</sub></td><td>c<sub>y</sub></td></tr>\n",
        "<tr><td>0</td><td>0</td><td>1</td></tr></table>\n"
      ],
      "metadata": {
        "id": "ETTChsnH3X-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distortion parameters are ($ k_1, k_2, k_3, p_1, p_2 $). Camera matrix contains focal length ($ f_x, f_y $) and optical centers ($ c_x, c_y $)."
      ],
      "metadata": {
        "id": "uGXLb2DsYw56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the calibration we need a chessboard like figure and more than ten photos from different directions.\n",
        "\n",
        "Let's download the images for calibration."
      ],
      "metadata": {
        "id": "ROL3RF4iB3oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O sample_data/cal.zip https://raw.githubusercontent.com/OSGeoLabBp/tutorials/master/english/img_processing/code/cal.zip\n",
        "!unzip -q -o sample_data/cal.zip -d sample_data"
      ],
      "metadata": {
        "id": "QDjZJmqS_bIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "width = 5       # Charuco board size\n",
        "height = 7\n",
        "board = cv2.aruco.CharucoBoard((width, height), .025, .0125, aruco_dict)   # generate board in memory\n",
        "img = cv2.aruco.CharucoBoard.generateImage(board, (500, 700))\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "_ = plt.title('Charuco board')"
      ],
      "metadata": {
        "id": "pppcVCZoGdfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first 6 images for calibration:"
      ],
      "metadata": {
        "id": "B1hPRvXwohgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 6, figsize=(15, 2))\n",
        "for i in range(6):\n",
        "    im = cv2.imread('sample_data/cal{:d}.jpg'.format(i+1))\n",
        "    ax[i].imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
        "    ax[i].set_title('cal{:d}.jpg'.format(i+1))"
      ],
      "metadata": {
        "id": "_kdOd6OKoqnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the ArUco calibration, let's find the camera matrix and the associated radial and tangential distortion parameters."
      ],
      "metadata": {
        "id": "aejJ9tbSBK6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allCorners = []\n",
        "allIds = []\n",
        "decimator = 0\n",
        "\n",
        "for name in glob.glob(\"sample_data/cal*.jpg\"):\n",
        "  frame = cv2.imread(name)\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "  corners, ids, _ = detector.detectMarkers(gray)\n",
        "  charucodetector = cv2.aruco.CharucoDetector(board)\n",
        "  corners1, ids1, _, _ = charucodetector.detectBoard(gray)\n",
        "  allCorners.append(corners1)\n",
        "  allIds.append(ids1)\n",
        "  decimator += 1\n",
        "\n",
        "ret, mtx, dist, rvecs, tvecs = cv2.aruco.calibrateCameraCharuco(allCorners, allIds, board, gray.shape, None, None)\n",
        "print(\"Camera matrix [pixels]\")\n",
        "for i in range(mtx.shape[0]):\n",
        "  print(f'{mtx[i][0]:8.1f} {mtx[i][1]:8.1f} {mtx[i][2]:8.1f}')\n",
        "print('Radial components')\n",
        "print(30 * '-')\n",
        "print(f'{dist[0][0]:10.5f} {dist[0][1]:10.5f} {dist[0][2]:10.5f}')\n",
        "print(30 * '-')\n",
        "print('Tangential components')\n",
        "print(f'{dist[0][3]:10.5f} {dist[0][4]:10.5f}')"
      ],
      "metadata": {
        "id": "-a57WaBDLnRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot undistorted image and the one corrected by calibration parameters."
      ],
      "metadata": {
        "id": "2hOozaUNRlKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gray = cv2.imread('sample_data/cal1.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
        "ax[0].imshow(gray, cmap='gray')\n",
        "ax[0].set_title('distorted image')\n",
        "ax[1].imshow(cv2.undistort(gray, mtx, dist, None), cmap='gray')\n",
        "_ = ax[1].set_title('undistorted image')"
      ],
      "metadata": {
        "id": "4RFlPEOWQnKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Complex example\n",
        "\n",
        "We have a video of a moving object with an ArUco marker. Let's process the video frame by frame and make a plot of movements. During the process images are corrected by the calibration data.\n",
        "\n",
        "[Click here](https://raw.githubusercontent.com/OSGeoLabBp/tutorials/master/english/img_processing/code/demo.mp4) to watch video.\n"
      ],
      "metadata": {
        "id": "BIPyr5UHsoi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O sample_data/demo.mp4 https://raw.githubusercontent.com/OSGeoLabBp/tutorials/master/english/img_processing/code/demo.mp4"
      ],
      "metadata": {
        "id": "JSJmRPVrLWHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture('sample_data/demo.mp4')\n",
        "frame = 0                 # frame counter\n",
        "xc = []                   # for pixel coordinates of marker\n",
        "yc = []\n",
        "frames = []\n",
        "while True:\n",
        "  ret, img = cap.read()   # get next frame from video\n",
        "  if ret:\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)        # convert image to grayscale\n",
        "    img_gray = cv2.undistort(gray, mtx, dist, None)     # remove camera distortion using calibration\n",
        "    corners, ids, _ = cv2.aruco.detectMarkers(img_gray, aruco_dict, parameters=params)  # find ArUco markers\n",
        "    if ids:                                             # marker found?\n",
        "      yc.append(img_gray.shape[1] - int(round(np.average(corners[0][0][:, 1]))))  # change y direction\n",
        "      frames.append(frame)\n",
        "    frame += 1                                          # frame count\n",
        "  else:\n",
        "    break       # no more images\n",
        "plt.plot(frames, yc)\n",
        "plt.title('Vertical positions of ArUco marker from video frames')\n",
        "plt.xlabel('frame count')\n",
        "plt.grid()\n",
        "_ = plt.ylabel('vertical position [pixel]')"
      ],
      "metadata": {
        "id": "eGHoz5wFMx9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Convert pixels to mm and frames to seconds (video has 2 fps, the size of the marker is 6.5 cm).*"
      ],
      "metadata": {
        "id": "C4F606fvbF57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tasks to do\n",
        "\n",
        "\n",
        "* Calibrate the camera of your laptop. Use e.g. windows built-in camera application to take images\n",
        "* Try *charuco.py* from Ulyxes to calibrate your camera and save the parameters\n",
        "* Take an image of an AruCo code using your laptop camera. Find its position using either template matching or Aruco finding methods\n",
        "* Take an image of more AruCo codes. Find their positions applying Aruco finding methods\n",
        "* Try *video_aruco.py* and *video_correlation.py* from Ulyxes. Use your laptop camera and find Aruoco code position in real-time.\n",
        "* Create a time, x or time, y graph from the output of *video_aruco.py* and *video_correlation.py*\n",
        "* Calibrate the camera of your mobile\n",
        "* Blur the images (monalisa.jpg, markers.png) and try to use both methods\n",
        "* Modify the code of the complex example to use template matching method instead of AruCo code recognition"
      ],
      "metadata": {
        "id": "DIcRBtw6p_oE"
      }
    }
  ]
}